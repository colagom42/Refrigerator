넓고 다양하게 사용되어, 실제 배포 및 사용의 중요성이 커짐.
 서버 설정이라든가 후속조치, 등등등... 이에 다양한 문제가 실제로 발생한다.

 이를 전체적으로 아우르는 과정 모두를 합쳐 파이프라인이라고 한다.

 케글을 사용한다.



 주피터 노트북에서 !치고 커맨드 치면 리눅스 명령어 실행됨


 MLP(multi layer perceptron)

 fully connected layer가 기초이다. 걍 여러개 쌓은것이다.


 Decision tree

 머신러닝 알고리즘 중 하나이다.
 impurity는 정보의 순도를 나타낸다. 얼마나 효율적으로 나눌 수 있느냐이다. gini계수가 대표적이다.
 높을수록 나무의 높은 단계에 넣는다. 낮을수록 좋다.
 적절한 깊이 조절이 필요하다.
 가지를 쳐서 지니계수가 오히려 높아지면, 안나누는게 더 정확하다.

 예제

 from sklearn.datasets import load_iris
 from sklearn import tree
iris = load_iris()

 X, y = iris.data, iris.target
 iris.feature_name

 model = tree.DecisionTreeClassifier()
 model = model.fit(X, y)
 import graphviz


 Ensemble model Random forest
 Baggig이다.
 트리를 겁나 만들어서 voting하겠다
 row를 1/3 정도만 잘라서 쓴다. cliumn은 classification에서 루트 컬럼, regression에서 1/3 컬럼정도 사용한다. 이렇게 조금씩 샘플링하여 종합한다.

 sklearn.ensemble.RandomForestClassifier이다.


 Boosting
 
 각 트리를 연속적으로 만든다. 가중평균이다. (트리마다 가중치 다름)
 1. 각 모델의 데이터셋?
 2. 각 트리의 가중치?

 어떤 부스팅이냐에 따라 다른데

 Adaboost는 
 1. 첫모델은 랜덤숲과 같이 랜덤으로 만들어짐.
 2. 에러 레이트에 따라 가중치를 준다. 잘맞추면 높고, 못맞추면 낮다못해 마이너스다.
 3. 이전 모델에서 예측을 벗어난 값들은 가능한 한 다음 모델에서 서브데이터셋에 포함되도록 한다.
 
 보통 XG부스트를 많이 쓴다.

 부스트는 더 정확하지만 과적합 위험이 높고 오래걸린다. 배깅은 그 반대다.
 boost는 .practice로 학습한다.

 모델 배포

 이래저래 배우었다으

 백엔드는 서버에 모델이 있어서 뿌려주기만 한다. 넷플릭스 추천도 비슷하다.
 지연이 발생하고, 서버 내꺼라 돈든다.
 대신 최적화 및 테스팅이 편하다.
 걍 뿌리기만하니깐
 엣지 디바이스 배포는 기기에 각각 배포한다.
 장단점은 정반대다


 ios core ML처럼 배포를 돕는 솔루션들도 있다.

 rest는 클라이언트-서버 통신 방식 중 하나로,
 http method를 이용한다.
 클라이언트 상태가 저장되지 않고, 계층화되어 보안성을 지닌다.


 REST API를 제공하면 RESTful 하다고 한다.
