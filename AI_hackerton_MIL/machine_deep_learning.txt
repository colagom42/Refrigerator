기본적인 설명

 머신러닝이란 조건문을 하나하나 적어주는게 아니라 기계가 알아서 학습하게 하는 것이다. 알파고가 그 예시라고 할 수 있다.

 바둑, 자율주행 등, 예외상황이 너무 많이 연역적 문제해결이 어려울 때 사용된다.

 Supervised는 정답이 존재하는, 개 고양이 분류등의 문제이다.
 Unsupervised learning은 군집화를 통해 분류한다. 노래, 영화등 컨텐츠에 많이 사용된다.

  알고리즘에 따라 군집화(clustering) 분류의 모양은 크게 다를 수 있다.


  python, tendorflow, keras등의 소프트웨어가 활용된다.



Training set 과 test set

 전자는 훈련용이고, 후자는 테스트용이며, 구별이 필수적인 이유는

 훈련용과 테스트용이 겹칠 경우 과적합이 발생하여 일반적 경향을 잘 반영하지 못할 수 있다.

 validation set은 overfitting을 막기 위해  test set 과 비슷하게 사용한다.














linear regression 선형회귀분석

구글 콜라보를 활용하면 좋다.
쉬프트 엔터가 실행
 
 정규화를 한다. 평균값을 뺀 것을 표준편차로 나눔. (normalize)

 행렬에 대해 선형 회귀분석하는 곱 행렬을 만들고, loss와 optimizer을 결정한다. 이에 따라 fit를 통해 모델을 학습시킨다.

INPUT
-----
dense 1
-----
dense 2
-----
.
.
.
-----
dense N
-----
OUTPUT

이런 느낌이다.



 epoch 만큼 반복하되, early stopping울 통해 loss와 val loss값의 차이를 보고 과적합을 방지할 수 있다.


비선형회귀분석

 activation function을 통해 비선형을 나타낸다. ReLu가 대표적.
 ReLu(WX + b)

 이걸 많이 쓰는 이유는 딥러닝에서 다른 함수는 레이어가 깊어질수록 효율이 낮아지기 때문.

 결과값을 변형시키기만 하는 레이어이다.

 댄스 레이어에 걍 추가시키면 단축시킬 수 있다.





classification

 범주형 데이터 예측을 classification이라고 한다.

 2차원 데이터는 flatten하여 사용한다.

 클래스별로 점수를 구현하기 위해 output을 10개로 한다.

 softmax layer를 통해 확률을 정규화한다.



unsupervised learning

 대충 뭔소리냐면

 수퍼바이즈드는 예측값과 실 값 사이의 loss 혹은 cost를 경사하향법으로 끝없이 교정하는것이다! XG부스트같은 소리지?

 언수퍼바이저는 오토인코더가 대표적인데, 본 데이터를 latent에 잘 압축시켜 나타내는 것이 중요하다는 소리다!
 나도 아직 뭔소린지 잘 모르겠다!
 내생각엔 경향을 잘 볼 수 있다는 점이 좋은 것 같다.


 오토인코더를 araboja

 supervised랑 비스무리하게 플래튼하고 댄스 레이어를 통해 점점 output을 줄이고, (encode) 이후 역작용하여 decode하자.

 입력과 출력 모두 x_train을 사용한다.


Deep learning


 Hyperparameter tunig

 수정되는 속도, 즉 learning rate를 조절할 수 있다.
 이는 W 와 b 가 수정되는 속도이다.

 이것이 대표적인 예시이다.

 너무 크면 적절한 값을 찾지 못한다. 너무 작으면 너무 오래 걸린다.
 동적으로 조절하면 좋지 않을까 했더니 진짜 좋았다.

 Learning rate scheduler를 통해 적절해 적절하다.

 Batch size는 한번에 트레이닝하는 데이터의 양이다. 기본값은 32다. 그니까 32개씩 공부한단 소리다. 이거에 따라서도 꽤 타이가 난다.

 최적화를 위해 grid search를 통해 찾아낸다. 개노가다다.

 랜덤서치 쓴다.

 AUTOML로 노가다를 줄인다. 편하다.

 KATIB openkeras 등 있다.


 과적합 피하기

 varience가 크고 bias가 작으면 overfitting, 그 반대를 underfitting이라고 함.

 1. 데이터 더 모은다. 데이터가 개애애애애애많으면 오버피팅 앵간해선 더 안뜬다.
 2. Early stopping. val loss 보고 차이가 너무 나면 그만 학습시킨다.
 3. Regularization ㅣL1, L2가 있고, W가 너무 크면 적당히 쳐내는 작업이다.
    추가적인 loss를 더해 불필요하다고 생각되는 가지를 쳐낸다.
 4.Dropouts 랜덤으로 레이어를 끊어버린다. 학습시에는 끊어서, 체크시에는 안끊고 한다. 이는 앙상블의 효과를 낸다.


 Weight initialization

 Weight는 trainable parameter이다.

 glorot uniform은 [-limit, limit] 사이의 uniform distribution으로, 이 때 limit는 sqrt(6/ (fan_in + fan_out)) (fan_in, out은 각각 dimension의 크기이다. 728, 128)

 모델이 작을수록 큰 영향을 미칠 수 있다.

 He Intialization은(0, Var(W)) 정규분포에서 var(W)를 sqrt(2/fan_in)으로 잡는다.


